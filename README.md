# ğŸ“Š CRM Analytics with Microsoft Fabric

This project simulates a modern CRM Analytics solution using the Microsoft Fabric platform. It involves generating synthetic data as well as integrating real-world data from a public API, storing it in a Lakehouse using the Medallion architecture, processing and transforming it with PySpark, leveraging Delta Lake versioning, automating pipelines, and building interactive dashboards with Power BI.

---

## ğŸš€ Technologies Used

- [Microsoft Fabric](https://www.microsoft.com/en-us/microsoft-fabric)
- Lakehouse (Delta Lake)
- Power BI
- PySpark (via Fabric notebooks)
- Fabric Pipelines
- Python (with Faker and requests)
- SQL (DAX, Spark SQL)

---

## ğŸ” Security and Access Management

This project follows best practices for data access governance using **RBAC (Role-Based Access Control)** and the **Principle of Least Privilege (PoLP)** to ensure secure and organized collaboration between different data roles in Microsoft Fabric.

1. **Create Azure Entra users**  
   Two internal users were created to simulate real enterprise roles:
   - [`dataEngineer@yourdomain.com`](imagens/security/user-data-engineer.png)
   - `dataAnalyst@yourdomain.com`  ![Data Analyst User](imagens/security/user-data-analyst.png)

2. **Create Azure Entra security groups**
   - `Data Engineers`: Group of Data Engineers responsible for designing, building, and maintaining scalable data pipelines, lakehouses, and transformations.
   - `Data Analysts`: Group of Data Analysts focused on exploring, interpreting, and visualizing business data to generate insights.

3. **Create dedicated workspaces in Fabric**
   - `CRM_Engineering_Dev`: Workspace for data engineering tasks â€“ ingestion and transformation of data (Bronze & Silver layers).
   - `CRM_Analytics_Dev`: Workspace for curated data and reporting â€“ Gold layer, semantic models, and Power BI dashboards.

4. **Assign roles to groups within workspaces**
   - In `CRM_Engineering_Dev`, the **Data Engineers** group was added as `Contributors`.
   - In `CRM_Analytics_Dev`, the **Data Analysts** group was added as `Contributors`.

---

## ğŸ›ï¸ Medallion Architecture

This project follows the Medallion data architecture pattern:

### ğŸ¥‰ Bronze â€“ Raw Layer
> Storage of raw data (synthetic and real), without transformations.

| Table               | Source                      | Description                             |
|----------------------|----------------------------|----------------------------------------|
| `bronze_customer`    | Faker                      | Synthetic customer records                     |
| `bronze_products`    | Faker                      | Products with categories and pricing       |
| `bronze_sales`      | Faker                      | Simulated sales history         |
| `bronze_ibge_states`| API IBGE                   | Official list of Brazilian states           |

---

### ğŸ¥ˆ Silver â€“ Clean Layer
> Cleaned data with standardized schemas, adjusted data types, and applied joins.

| Table               | Description                          |
|----------------------|------------------------------------|
| `dim_customer`        | Customer dimension               |
| `dim_product`        | Product dimension               |
| `dim_time`          | Time dimension (2 years range)         |
| `dim_geographic`       | Standardized geographic dimension    |
| `fact_sales`        | 	Fact table with keys and metrics         |

---

### ğŸ¥‡ Gold â€“ Business Layer
> Final business-ready data for aggregations and key indicators.

| Table                   | Description                           |
|--------------------------|-------------------------------------|
| `total_sales_state`   | Total sales by state          |
| `top_customers`        | Top customers by volume      |
| `top_products`           | Best-selling products              |

---

## ğŸ” Security

- **Workspace-level access control** in Microsoft Fabric
- **Data masking for sensitive fields**, such as customer emails
- **Layer-based access separation**: full access to Bronze/Silver, read-only access to Gold
- Ready for **auditing and monitoring** via Fabric or Azure Purview

---

## ğŸ•’ Data Versioning with Delta Time Travel

This project uses Delta Lake as the storage format, allowing automatic versioning of all tables. This enables:

- Restore previous versions of any table
- Audit transformations and load history
- Re-run analyses based on historical versions

Example usage in a notebook:

```sql
-- Query a previous version of the sales fact table (SQL)
SELECT * FROM silver.fato_vendas VERSION AS OF 3
```
```python
# Query a previous version using PySpark
df_v2 = spark.read.format("delta") \
    .option("versionAsOf", 3) \
    .load("Tables/silver/fact_sales")
```

## âš™ï¸ Pipeline Automation

All ETL processes are automated using Microsoft Fabric Pipelines.

ğŸ” Created Pipelines

| Pipeline                   | Steps Description                           |
|--------------------------|-------------------------------------|
| `etl_bronze_to_silver`   | Cleaning, casting, joins, write to Silver          |
| `etl_silver_to_gold`     | Aggregations and final indicators generation      |
| `api_collection_ibge`           | Real-time data collection from IBGE API              |

Each pipeline can be scheduled (e.g., daily) or triggered manually.

---

## ğŸ“Š Power BI Dashboards

- ğŸ—ºï¸ Sales map by state (map and bar chart)
- ğŸ“¦ Top-selling products
- ğŸ‘¥ Most active customers
- ğŸ“‰ Sales trend over time (line chart)

> Dashboards were built directly within Power BI in Fabric, using native Lakehouse connections.

ğŸ“· Dashboard screenshots are available in /imagens/dashboard.png.

---

## âš¡ Performance Best Practices

- Storage in Delta Lake (compression + versioning)
- Table partitioning in Silver and Gold layers (e.g., by state or date)
- Reusable notebook modules
- Clear separation of responsibilities across layers
- Incremental ETL with future timestamp control (optional)

---

## ğŸ“ Project Structure

```plaintext
crm-analytics-fabric/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ geracao_dados.ipynb
â”‚   â”œâ”€â”€ coleta_api_ibge.ipynb
â”‚   â”œâ”€â”€ bronze_to_silver.ipynb
â”‚   â””â”€â”€ silver_to_gold.ipynb
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ etl_bronze_to_silver.json
â”‚   â”œâ”€â”€ etl_silver_to_gold.json
â”‚   â””â”€â”€ api_coleta_ibge.json
â”œâ”€â”€ modelagem/
â”‚   â””â”€â”€ modelo_dimensional.drawio
â”œâ”€â”€ imagens/
â”‚   â””â”€â”€ dashboard.png
â”œâ”€â”€ README.md
```
---

## ğŸ§  Demonstrated Skills

- âœ”ï¸ Dimensional modeling (star schema)
- âœ”ï¸ Lakehouse architecture
- âœ”ï¸ Medallion architecture (Bronze â†’ Silver â†’ Gold)
- âœ”ï¸ ETL with PySpark
- âœ”ï¸ Delta Time Travel (data versioning)
- âœ”ï¸ Automation with Fabric Pipelines
- âœ”ï¸ Data visualization with Power BI
- âœ”ï¸ Security and performance best practices

---

**Author:** Danillo Oliveira  
**LinkedIn:** https://www.linkedin.com/in/danillobsoliveira/ 
